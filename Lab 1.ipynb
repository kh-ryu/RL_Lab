{"cells":[{"cell_type":"markdown","metadata":{"id":"PO6vGmAtMKOh"},"source":["# Lab 1: Value Iteration"]},{"cell_type":"markdown","metadata":{"id":"yXZCyKzgMKOi"},"source":["*OpenAI gym FrozenLake environment*\n","\n","Winter is here. You and your friends were tossing around a frisbee at the park\n","    when you made a wild throw that left the frisbee out in the middle of the lake.\n","    The water is mostly frozen, but there are a few holes where the ice has melted.\n","    If you step into one of those holes, you'll fall into the freezing water.\n","    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n","    you navigate across the lake and retrieve the disc.\n","    However, the ice is slippery, so you won't always move in the direction you intend.\n","    The surface is described using a grid like the following\n","\n","        SFFF\n","        FHFH\n","        FFFH\n","        HFFG\n","\n","    S : starting point, safe\n","    F : frozen surface, safe\n","    H : hole, fall to your doom\n","    G : goal, where the frisbee is located\n","\n","    The episode ends when you reach the goal or fall in a hole.\n","    You receive a reward of 1 if you reach the goal, and zero otherwise.\n","   "]},{"cell_type":"markdown","metadata":{"id":"LC-IV7YmMKOj"},"source":["In this assignment, you will be implementing a value iteration algorithm"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1647844967797,"user":{"displayName":"­황태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08611587733822633308"},"user_tz":-540},"id":"WMRPWkoXMKOj","outputId":"a20994d0-31e7-4264-8397-e9f8543811c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["numpy version 1.21.5\n","gym version 0.17.3\n"]}],"source":["import gym\n","import numpy as np\n","print(\"numpy version\", np.__version__)\n","print(\"gym version\", gym.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1647844969492,"user":{"displayName":"­황태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08611587733822633308"},"user_tz":-540},"id":"99HHdrBXMKOk"},"outputs":[],"source":["env=gym.make('FrozenLake-v0')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1647844970878,"user":{"displayName":"­황태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08611587733822633308"},"user_tz":-540},"id":"D4uizPB8MKOk"},"outputs":[],"source":["## DO NOT CHANGE THIS CELL\n","\n","def argmax(env, V, pi, action,s, gamma):\n","    e = np.zeros(env.env.nA)\n","    for a in range(env.env.nA):                         # iterate for every action possible \n","        q=0\n","        P = np.array(env.env.P[s][a])                   \n","        (x,y) = np.shape(P)                             # for Bellman Equation \n","        \n","        for i in range(x):                              # iterate for every possible states\n","            s_= int(P[i][1])                            # S' - Sprime - possible succesor states\n","            p = P[i][0]                                 # Transition Probability P(s'|s,a) \n","            r = P[i][2]                                 # Reward\n","            \n","            q += p*(r+gamma*V[s_])                      # calculate action_ value q(s|a)\n","            e[a] = q\n","            \n","    m = np.argmax(e) \n","    action[s]=m                                         # Take index which has maximum value \n","    pi[s][m] = 1                                        # update pi(a|s) \n","\n","    return pi"]},{"cell_type":"markdown","metadata":{"id":"GwJD1lBDMKOk"},"source":["Your task is to complete the \"bellman_opt_update\" function below, which servces as a subroutine for the value iteration method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1IObFjBMKOl"},"outputs":[],"source":["def bellman_opt_update(env, V, s, gamma):  # update the stae_value V[s] by taking \n","\n","    #########################\n","    # Place your code here\n","    ##########################\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1647844974897,"user":{"displayName":"­황태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08611587733822633308"},"user_tz":-540},"id":"bn6uxXw0MKOl"},"outputs":[],"source":["## DO NOT CHANGE THIS CELL\n","\n","def value_iteration(env, gamma, theta):\n","    V = np.zeros(env.env.nS)                              # initialize v(0) to arbitory value, my case \"zeros\"\n","    while True:\n","        delta = 0\n","        for s in range(env.env.nS):                       # iterate for all states\n","            v = V[s]\n","            bellman_opt_update(env, V, s, gamma)   # update state_value with bellman optimality update\n","            delta = max(delta, abs(v - V[s]))             # assign the change in value per iteration to delta  \n","        if delta < theta:                                       \n","            break                                         # if change gets to negligible \n","                                                          # --> converged to optimal value         \n","    pi = np.zeros((env.env.nS, env.env.nA)) \n","    action = np.zeros((env.env.nS))\n","    for s in range(env.env.nS):\n","        pi = argmax(env, V, pi, action, s, gamma)         # extract optimal policy using action value \n","        \n","    return V, pi, action   # optimal value funtion, optimal policy with one-hot encoding (pi), optimal policy with discrete number (action)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1647844977035,"user":{"displayName":"­황태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08611587733822633308"},"user_tz":-540},"id":"_teSxydCMKOl","outputId":"c42846ad-ca37-4711-e968-54cd474a7262"},"outputs":[{"name":"stdout","output_type":"stream","text":[" agent succeeded to reach goal 1 out of 1000 Episodes using this policy \n"," success rate: 0.001\n"]}],"source":["## DO NOT CHANGE THIS CELL\n","\n","gamma = 0.99\n","theta = 0.000001\n","\n","env.seed(1)\n","V, pi, action = value_iteration(env, gamma, theta) # note \"action\" is optimal policy with discrete action number\n","\n","#initialize episodic structure\n","num_episodes=1000;\n","episode_max_length=10000;\n","\n","\n","e=0\n","for i_episode in range(num_episodes):\n","    s = env.reset()\n","    for t in range(episode_max_length):\n","        s, reward, done, _ = env.step(action[s])\n","        if done:\n","            if reward == 1:\n","                e +=1\n","            break\n","print(\" agent succeeded to reach goal {} out of {} Episodes using this policy \".format(e+1, num_episodes))\n","print(\" success rate:\", (e+1)/num_episodes)\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"3xKSiGBZMKOm"},"source":["### Save this notebook file (after you modify and run the code) and submit \"lab1.ipynb\" on eTL\n","\n","Your success rate should be above 70%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uo5D6lp4MKOm"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Lab 1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
